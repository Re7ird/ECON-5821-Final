---
title: "Final5821"
author: "re7ird"
date: "2023-05-02"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(glmnet)
set.seed(10)
```

```{r}
load(url("https://github.com/zhentaoshi/Econ5821/raw/main/data_example/dataset_inf.Rdata"))

```

Perequisite

```{r}
data_1=merge(cpi,X,by="month",all = T)
data_1=data_1 |> mutate(y=lead(CPI)-100)
```

## Model: OLS

```{r}
data_Lasso_train=data_1[1:130,]
data_Lasso_test=data_1[131:167,]

y <- data_Lasso_train$y
yt <- data_Lasso_test$y
x <- data_Lasso_train |> select(!c(month,CPI,y))
x=x[,1:10]
xn=data_Lasso_test |> select(!c(month,CPI,y))
xn=xn[,1:10]

ols_data=cbind(y,x)

ols1=lm(y~.,data=ols_data)
summary(ols1)

#find R-squared of model on training data
y_predicted <- predict(ols1, newdata = xn)

sst <- sum((yt - mean(yt))^2)
sse <- sum((y_predicted - yt)^2)

rsq <- 1 - sse/sst
rsq
```

## Model2: A Simple Lasso Model.

```{r}
#Define predictor and response variables

data_Lasso_train=data_1[1:130,]
data_Lasso_test=data_1[131:167,]
y <- data_Lasso_train$y[1:130]
yt <- data_Lasso_test$y
x <- data_Lasso_train |> select(!c(month,CPI,y))
xn=as.matrix(data_Lasso_test |> select(!c(month,CPI,y)))
xx=as.matrix(x)

#fit lasso regression model using k-fold cross-validation
cv_model <- cv.glmnet(xx, y, alpha = 1)
best_lambda <- cv_model$lambda.min

#fix best lambda
best_lambda=0.02332975

#display optimal lambda value
best_lambda

#view plot of test MSE's vs. lambda values
plot(cv_model)

#view coefficients of best model
best_model <- glmnet(x, y, alpha = 1, lambda = best_lambda)
coef(best_model)

#make a prediction for the response value of a new observation

predict(best_model, s = best_lambda, newx = xn)

#find R-squared of model on training data
y_predicted <- predict(best_model, s = best_lambda, newx = xn)

sst <- sum((yt - mean(yt))^2)
sse <- sum((y_predicted - yt)^2)

rsq <- 1 - sse/sst
rsq
```

## Model3: PCA with Lasso

One idea: dimension reduction:

PCA to see if they're correlated

```{r}
pca_data=data_1 |> select(!c("month","y"))

mean(cor(pca_data))
PCA=princomp(pca_data)
summary(PCA)

```

We can see that the first 8 components explained 95% variances.

```{r}
loadings=PCA$loadings[,1:8]
PC=PCA$scores[,1:8]



```

The idea is use rolling windows to train and test data. That is, use 12 month data to train the model and predict next month.

```{r}
pca_lasso_test <- function(i,data) {
  j=i+11
  k=i+12
  y_train=data[i:j,"y"]
  y_test=data[k,"y"]
  x_train=data[i:j,] |> select(!c("month","y"))
  x_test=data[k,] |> select(!c("month","y"))
  
  #Train the PCA model
  pca_data=data |> select(!c("month","y","CPI"))
  PCA=princomp(pca_data)
  
  #collect 8 Principal Components and train the lasso regression.
  PC=predict(PCA,newdata = x_train)[,1:8]
  
  #Calculate scores for testin
  PC_test=predict(PCA,newdata = x_test)[,1:8]
  
  #find lambda
  cv_model <- cv.glmnet(as.matrix(x_train), y_train, alpha = 1,nfolds = 3)
  best_lambda <- cv_model$lambda.min
  
  #display optimal lambda value
  best_lambda
  
  #view plot of test MSE's vs. lambda values
  
  #view coefficients of best model
  pca_lasso <- glmnet(PC, y_train, alpha = 1, lambda =best_lambda)
  coef(best_model)
  summary(pca_lasso)
  
  #make a prediction for the response value of a new observation
  yp=predict(pca_lasso, s = best_lambda, newx = PC_test)[1,1]
  return(c(yp,y_test))
  
}

rsquare_test <- function(yt,y_predicted) {
  sst <- sum((yt - mean(yt))^2)
  sse <- sum((y_predicted - yt)^2)
  
  rsq <- 1 - sse/sst

 cat("The R-square is:",rsq,"\n") 
 return(rsq)
}
```

```{r}
#Test

y_predicted=vector()
yt=vector()

for (x in 1:155) {
  results=pca_lasso_test(i=x,data_1)
  y_test = results[2]
  yp=results[1]
  y_predicted = append(y_predicted,yp)
  yt=append(yt,y_test)
}

rsquare_test(yt,y_predicted)
```

Lag=2:

```{r}
#Lag=2:
data_12=merge(cpi,X,by="month",all = T)
data_12=data_12 |> mutate(y=lead(CPI,n=2)-100) |> filter(is.na(y)==F)

y_predicted=vector()
yt=vector()

for (x in 1:150) {
  results=pca_lasso_test(i=x,data_12)
  y_test = results[2]
  yp=results[1]
  y_predicted = append(y_predicted,yp)
  yt=append(yt,y_test)
}

sst <- sum((yt - mean(yt))^2)
sse <- sum((y_predicted - yt)^2)

rsq <- 1 - sse/sst
rsq


```

Lag=3

```{r}
#Lag=3:
data_13=merge(cpi,X,by="month",all = T)
data_13=data_13 |> mutate(y=lead(CPI,n=3)-100) |> filter(is.na(y)==F)

y_predicted=vector()
yt=vector()

for (x in 1:150) {
  results=pca_lasso_test(i=x,data_13)
  y_test = results[2]
  yp=results[1]
  y_predicted = append(y_predicted,yp)
  yt=append(yt,y_test)
}

sst <- sum((yt - mean(yt))^2)
sse <- sum((y_predicted - yt)^2)

rsq <- 1 - sse/sst
rsq
```

Seems like n=2 is the best model. That is, use month t to predict t+2.

What if I add CPI of the month to predict next month?

```{r}
pca_lasso_test2 <- function(i,data) {
  j=i+11
  k=i+12
  y_train=data[i:j,"y"]
  y_test=data[k,"y"]
  x_train=data[i:j,] |> select(!c("month","y"))
  x_test=data[k,] |> select(!c("month","y"))
  
  #Train the PCA model
  pca_data=data |> select(!c("month","y","CPI"))
  PCA=princomp(pca_data)
  
  #collect 8 Principal Components and train the lasso regression.
  PC=predict(PCA,newdata = x_train)[,1:8]
  
  #Calculate scores for testin
  PC_test=predict(PCA,newdata = x_test)[,1:8]
  
  #find lambda
  cv_model <- cv.glmnet(as.matrix(x_train), y_train, alpha = 1,nfolds = 3)
  best_lambda <- cv_model$lambda.min
  
  #display optimal lambda value
  best_lambda
  
  #view plot of test MSE's vs. lambda values
  
  #view coefficients of best model
  PCs = cbind(PC,x_train$CPI)
  pca_lasso2 <- glmnet(PCs, y_train, alpha = 1, lambda =best_lambda)
  coef(pca_lasso2)
  summary(pca_lasso2)
  
  #make a prediction for the response value of a new observation
  PC_test2=append(PC_test,x_test$CPI)
  yp=predict(pca_lasso2, s = best_lambda, newx = PC_test2)[1,1]
  return(c(yp,y_test))
  
}


y_predicted=vector()
yt=vector()

for (x in 1:150) {
  results=pca_lasso_test2(i=x,data_1)
  y_test = results[2]
  yp=results[1]
  y_predicted = append(y_predicted,yp)
  yt=append(yt,y_test)
}



sst <- sum((yt - mean(yt))^2)

sse <- sum((y_predicted - yt)^2)

rsq <- 1 - sse/sst
rsq
```

### Repeated test

```{r}
rep_test1 <- function(data){
  y_predicted=vector()
  yt=vector()

  for (x in 1:150) {
    results=pca_lasso_test(i=x,data_13)
    y_test = results[2]
    yp=results[1]
    y_predicted = append(y_predicted,yp)
    yt=append(yt,y_test)
  }
  rsquare_test(yt,y_predicted = y_predicted)
  
}

rsqv=c()
for (t in 1:5){

  rsqv=append(rsqv,rep_test1(data_1))
  
}
mean(rsqv)
sd(rsqv)
```
